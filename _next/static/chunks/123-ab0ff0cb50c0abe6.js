"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[123],{6690:function(e,t,n){n.d(t,{Z:function(){return o}});var i=n(5893),a=n(7294);function o(){let[e,t]=(0,a.useState)(!1);return((0,a.useEffect)(()=>{let e=()=>{window.pageYOffset>300?t(!0):t(!1)};return window.addEventListener("scroll",e),()=>window.removeEventListener("scroll",e)},[]),e)?(0,i.jsx)("button",{onClick:()=>{window.scrollTo({top:0,behavior:"smooth"})},className:"fixed bottom-8 right-8 p-3 bg-accent text-accent-foreground rounded-full shadow-lg hover:bg-accent/90 transition-all duration-200 z-50 hover:shadow-xl transform hover:-translate-y-1","aria-label":"Back to top",children:(0,i.jsx)("svg",{className:"w-5 h-5",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:(0,i.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M5 15l7-7 7 7"})})}):null}},9793:function(e,t,n){n.d(t,{Z:function(){return d}});var i=n(5893),a=n(1664),o=n.n(a),r=n(7294);function s(){let[e,t]=(0,r.useState)(!1),[n,a]=(0,r.useState)(!1);return((0,r.useEffect)(()=>{a(!0);let e=localStorage.getItem("theme"),n=window.matchMedia("(prefers-color-scheme: dark)").matches,i="dark"===e||!e&&n;t(i),i?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")},[]),n)?(0,i.jsx)("button",{onClick:()=>{let n=!e;t(n),n?(document.documentElement.classList.add("dark"),localStorage.setItem("theme","dark")):(document.documentElement.classList.remove("dark"),localStorage.setItem("theme","light"))},className:"p-2 rounded-md hover:bg-muted/50 transition-colors","aria-label":e?"Switch to light mode":"Switch to dark mode",children:e?(0,i.jsx)("svg",{className:"w-4 h-4",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:(0,i.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"})}):(0,i.jsx)("svg",{className:"w-4 h-4",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:(0,i.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z"})})}):(0,i.jsx)("button",{className:"p-2 rounded-md hover:bg-muted/50 transition-colors","aria-label":"Toggle theme",children:(0,i.jsx)("div",{className:"w-4 h-4"})})}function l(){let[e,t]=(0,r.useState)(0);return(0,r.useEffect)(()=>{let e=()=>{t(Math.min(100,Math.max(0,window.scrollY/(document.documentElement.scrollHeight-window.innerHeight)*100)))};return window.addEventListener("scroll",e),e(),()=>window.removeEventListener("scroll",e)},[]),(0,i.jsx)("div",{className:"fixed top-0 left-0 w-full h-1 bg-border/20 z-40",children:(0,i.jsx)("div",{className:"h-full bg-gradient-to-r from-accent to-accent/60 transition-all duration-300",style:{width:"".concat(e,"%")}})})}let c=[{href:"/about",label:"About",isPage:!0},{href:"/blog",label:"Blog",isPage:!0},{href:"#contact",label:"Contact",isPage:!1}];function d(e){let{name:t}=e,[n,a]=(0,r.useState)(!1),[d,u]=(0,r.useState)(""),[h,g]=(0,r.useState)(!1);(0,r.useEffect)(()=>{let e=()=>{g(window.scrollY>20);let e=c.map(e=>e.href.substring(1)).find(e=>{let t=document.getElementById(e);if(t){let e=t.getBoundingClientRect();return e.top<=100&&e.bottom>=100}return!1});e&&u(e)};return window.addEventListener("scroll",e),e(),()=>window.removeEventListener("scroll",e)},[]);let m=e=>{let t=document.querySelector(e);t&&t.scrollIntoView({behavior:"smooth",block:"start"})};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("nav",{className:"fixed top-0 w-full z-50 transition-all duration-300 ".concat(h?"bg-background/80 backdrop-blur-md border-b border-border/50":"bg-transparent"),children:(0,i.jsxs)("div",{className:"max-width-container section-padding py-4",children:[(0,i.jsxs)("div",{className:"flex items-center justify-between",children:[(0,i.jsx)(o(),{href:"/",className:"text-lg font-serif font-semibold transition-colors duration-200 hover:text-accent ".concat("text-foreground"),children:t}),(0,i.jsxs)("div",{className:"hidden md:flex items-center space-x-1",children:[c.map(e=>e.isPage?(0,i.jsx)(o(),{href:e.href,className:"px-3 py-2 text-sm transition-colors duration-200 rounded-md text-muted-foreground hover:text-foreground hover:bg-muted/50",children:e.label},e.href):(0,i.jsx)("button",{onClick:()=>m(e.href),className:"px-3 py-2 text-sm transition-colors duration-200 rounded-md ".concat(d===e.href.substring(1)?"text-accent bg-accent/10":"text-muted-foreground hover:text-foreground hover:bg-muted/50"),children:e.label},e.href)),(0,i.jsx)(s,{})]}),(0,i.jsxs)("div",{className:"md:hidden flex items-center space-x-1",children:[(0,i.jsx)(s,{}),(0,i.jsx)("button",{className:"p-2 rounded-md hover:bg-muted/50 transition-colors",onClick:()=>a(!n),"aria-label":"Toggle menu",children:(0,i.jsx)("svg",{className:"w-5 h-5",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:n?(0,i.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M6 18L18 6M6 6l12 12"}):(0,i.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M4 6h16M4 12h16M4 18h16"})})})]})]}),n&&(0,i.jsx)("div",{className:"md:hidden mt-4 pb-4 space-y-1 border-t border-border/20 pt-4",children:c.map(e=>e.isPage?(0,i.jsx)(o(),{href:e.href,className:"block w-full text-left py-2 px-3 text-sm rounded-md transition-colors text-muted-foreground hover:text-foreground hover:bg-muted/50",onClick:()=>a(!1),children:e.label},e.href):(0,i.jsx)("button",{onClick:()=>{m(e.href),a(!1)},className:"block w-full text-left py-2 px-3 text-sm rounded-md transition-colors ".concat(d===e.href.substring(1)?"text-accent bg-accent/10":"text-muted-foreground hover:text-foreground hover:bg-muted/50"),children:e.label},e.href))})]})}),(0,i.jsx)(l,{}),(0,i.jsx)("div",{className:"h-16"})]})}},5038:function(e,t,n){n.d(t,{portfolioData:function(){return i}});let i={personal:{name:"Juyoung Suk",title:"M.S. Student in Artificial Intelligence",email:"juyoung@kaist.ac.kr",github:"https://github.com/juyoungml",linkedin:"https://www.linkedin.com/in/juyoung-suk-b5175a192/",googleScholar:"#",profileImage:"/profile.jpeg",bio:"I am a graduate student at KAIST researching machine learning and artificial intelligence, advised by Minjoon Seo. My work focuses on language model evaluation, multilingual AI systems, and developing novel approaches to deep learning architectures."},about:{paragraphs:["I am currently pursuing my Master's degree in Computer Science, where I work on advancing the state-of-the-art in machine learning. My research interests span neural architecture search, interpretable AI, and efficient deep learning.","Prior to graduate school, I completed my B.S. in Computer Science with a focus on artificial intelligence and data science. I have industry experience working on large-scale ML systems and have contributed to several open-source projects."]},experience:[{id:1,role:"Member of Technical Staff",company:"Trillion Labs",period:"Nov. 2024 - Present",description:"Core developer of Trillion-7B multilingual LLM.",highlights:["Trillion-7B development (HuggingFace, NVIDIA GTC)"]},{id:2,role:"Machine Learning Engineer",company:"ThetaOne",period:"Feb. 2023 - July. 2023",description:"RAG pipeline development for Metabuddy.",highlights:["End-to-end ML pipeline with LangChain"]},{id:3,role:"Machine Learning Engineer Intern",company:"NAVER Corp.",period:"Aug. 2022 - Feb. 2023",description:"Hate speech detection for AI Clean Bot 2.0.",highlights:["40+ million user production model"]}],projects:[{id:1,title:"Prometheus-Eval",description:"Led development of a 900+ stars open-source repository for evaluating language models using specialized LMs",technologies:["Python","PyTorch","HuggingFace","OpenAI API"],github:"https://github.com/prometheus-eval/prometheus-eval",demo:"#",featured:!0},{id:3,title:"Federated Learning Platform",description:"Privacy-preserving distributed learning platform for healthcare applications",technologies:["Python","TensorFlow","gRPC","Kubernetes"],github:"https://github.com/username/fed-learning",featured:!1},{id:4,title:"AI Code Assistant",description:"VS Code extension for AI-powered code completion and refactoring suggestions",technologies:["TypeScript","Node.js","OpenAI API","VS Code API"],github:"https://github.com/username/ai-code-assistant",demo:"#",featured:!1}],publications:[{id:1,title:"Prometheus 2: An open source language model specialized in evaluating other language models",authors:"S Kim, J Suk, S Longpre, BY Lin, J Shin, S Welleck, G Neubig, M Lee, ...",venue:"Preprint",year:2024,abstract:"Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ran",links:{paper:"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=mENsLCkAAAAJ&citation_for_view=mENsLCkAAAAJ:u-x6o8ySG0sC",arxiv:"https://arxiv.org/abs/2405.01535",code:"https://github.com/prometheus-eval/prometheus-eval"}},{id:2,title:"CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean",authors:"E Kim, J Suk, P Oh, H Yoo, J Thorne, A Oh",venue:"Preprint",year:2024,abstract:"Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and h",links:{paper:"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=mENsLCkAAAAJ&citation_for_view=mENsLCkAAAAJ:u5HHmVD_uO8C",arxiv:"https://arxiv.org/abs/2403.06412",code:"https://github.com/rladmstn1714/CLIcK"}},{id:3,title:"The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models",authors:"S Kim, J Suk, JY Cho, S Longpre, C Kim, D Yoon, G Son, Y Cho, ...",venue:"Preprint",year:2024,abstract:"As language models (LMs) become capable of handling a wide range of tasks, their evaluation is becoming as challenging as their development. Most generation benchmarks currently assess LMs using abstract evaluation criteria like helpfulness and harmlessness, which often lack the flexibility and granularity of human assessment. Additionally, these benchmarks tend to focus disproportionately on specific capabilities such as instruction following, leading to coverage bias. To overcome these limitat",links:{paper:"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=mENsLCkAAAAJ&citation_for_view=mENsLCkAAAAJ:d1gkVwhDpl0C",arxiv:"https://arxiv.org/abs/2406.05761",code:"https://github.com/prometheus-eval/prometheus-eval/tree/main/BiGGen-Bench"}},{id:4,title:"Evaluating Language Models as Synthetic Data Generators",authors:"S Kim, J Suk, X Yue, V Viswanathan, S Lee, Y Wang, K Gashteovski, ...",venue:"Preprint",year:2024,abstract:"Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data gener",links:{paper:"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=mENsLCkAAAAJ&citation_for_view=mENsLCkAAAAJ:2osOgNQ5qMEC",arxiv:"https://arxiv.org/abs/2412.03679",code:"https://github.com/neulab/data-agora"}},{id:5,title:"LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation",authors:"E Kim, J Suk, S Kim, N Muennighoff, D Kim, A Oh",venue:"Preprint",year:2024,abstract:"We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large language models (LLMs). This approach leverages multi-turn interactions where the LLM interviewer actively provides feedback on responses and poses follow-up questions to the evaluated LLM. At the start of the interview, the LLM interviewer dynamically modifies datasets to generate initial questions, mitigating data contamination. We apply the LLM-as-an-Interviewer framework to evaluate six models on the MATH and DepthQA t",links:{paper:"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=mENsLCkAAAAJ&citation_for_view=mENsLCkAAAAJ:UeHWp8X0CEIC",arxiv:"https://arxiv.org/abs/2412.10424"}},{id:6,title:"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",authors:"G Son, D Yoon, J Suk, J Aula-Blasco, M Aslan, VT Kim, SB Islam, ...",venue:"Preprint",year:2024,abstract:"As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator L",links:{paper:"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=mENsLCkAAAAJ&citation_for_view=mENsLCkAAAAJ:9yKSN-GCB0IC",arxiv:"https://arxiv.org/abs/2410.17578"}},{id:7,title:"Trillion 7B Technical Report",authors:"S Han, J Suk, S An, H Kim, K Kim, W Yang, S Choi, J Shin",venue:"Technical Whitepaper",year:2025,abstract:"We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10% of its 2T training tokens to multilingual data and r",links:{paper:"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=mENsLCkAAAAJ&citation_for_view=mENsLCkAAAAJ:qjMakFHDy7sC",arxiv:"https://arxiv.org/abs/2504.15431"}}],research:[{id:1,title:"Efficient Deep Learning",description:"Developing methods to reduce the computational and memory requirements of deep neural networks while maintaining accuracy.",icon:"⚡"},{id:2,title:"Interpretable AI",description:"Creating techniques to understand and explain the decision-making processes of complex machine learning models.",icon:"\uD83D\uDD0D"},{id:3,title:"Vision-Language Models",description:"Exploring the intersection of computer vision and natural language processing for multimodal understanding.",icon:"\uD83D\uDC41️"},{id:4,title:"Federated Learning",description:"Investigating privacy-preserving machine learning techniques for distributed and decentralized training scenarios.",icon:"\uD83D\uDD10"}],news:[{id:1,date:"2025-01",content:"\uD83D\uDE80 Released Trillion-7B Technical Whitepaper detailing our compute-efficient multilingual frontier model.",link:"https://arxiv.org/abs/2504.15431"},{id:2,date:"2024-12",content:'\uD83D\uDCDD Two new papers released: "LLM-AS-AN-INTERVIEWER" introducing dynamic evaluation paradigm and "Evaluating Language Models as Synthetic Data Generators" with AgoraBench.'},{id:3,date:"2024-11",content:"\uD83C\uDF89 Started position as Member of Technical Staff at Trillion Labs, working on next-generation multilingual language models."},{id:4,date:"2024-10",content:"\uD83C\uDF10 Released MM-Eval: A comprehensive multilingual meta-evaluation benchmark for LLM-as-a-Judge and reward models.",link:"https://arxiv.org/abs/2410.17578"},{id:5,date:"2024-06",content:"\uD83C\uDFC6 BiGGen Bench receives recognition as Best Paper - our principled benchmark for fine-grained LLM evaluation.",link:"https://arxiv.org/abs/2406.05761"},{id:6,date:"2024-05",content:"⭐ Prometheus 2 open-sourced with 900+ GitHub stars - specialized language model for evaluating other LLMs.",link:"https://arxiv.org/abs/2405.01535"}],skills:{languages:["Python","TypeScript","C++","Java","SQL"],frameworks:["PyTorch","TensorFlow","JAX","React","Next.js"],tools:["Docker","Kubernetes","AWS","Git","Linux"],ml:["Deep Learning","Computer Vision","NLP","Reinforcement Learning","MLOps"]}}}}]);