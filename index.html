<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>Juyoung Suk</title><meta name="description" content="Juyoung Suk - M.S. Student in Artificial Intelligence"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="5"/><link rel="preload" href="/_next/static/css/966be810721f038f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/966be810721f038f.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-135848b527a9a706.js" defer=""></script><script src="/_next/static/chunks/pages/_app-26f5890917895004.js" defer=""></script><script src="/_next/static/chunks/664-d254d21a6fe56bff.js" defer=""></script><script src="/_next/static/chunks/pages/index-fbdaccef2c546f8d.js" defer=""></script><script src="/_next/static/-HGjvzzIxlnwRl8I8McRs/_buildManifest.js" defer=""></script><script src="/_next/static/-HGjvzzIxlnwRl8I8McRs/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="min-h-screen bg-background"><nav class="fixed top-0 w-full z-50 transition-all duration-300 bg-transparent"><div class="max-width-container section-padding py-4"><div class="flex items-center justify-between"><a class="text-lg font-serif font-semibold transition-colors duration-200 hover:text-accent text-foreground" href="/">Juyoung Suk</a><div class="hidden md:flex items-center space-x-1"><button class="px-3 py-2 text-sm transition-colors duration-200 rounded-md text-muted-foreground hover:text-foreground hover:bg-muted/50">News</button><button class="px-3 py-2 text-sm transition-colors duration-200 rounded-md text-muted-foreground hover:text-foreground hover:bg-muted/50">Projects</button><button class="px-3 py-2 text-sm transition-colors duration-200 rounded-md text-muted-foreground hover:text-foreground hover:bg-muted/50">Publications</button><button class="ml-2 px-4 py-2 text-sm bg-accent text-accent-foreground rounded-md hover:bg-accent/90 transition-colors duration-200">Contact</button></div><button class="md:hidden p-2 rounded-md hover:bg-muted/50 transition-colors" aria-label="Toggle menu"><svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="fixed top-0 left-0 w-full h-1 bg-border/20 z-40"><div class="h-full bg-gradient-to-r from-accent to-accent/60 transition-all duration-300" style="width:0%"></div></div><div class="h-16"></div><main class="max-width-container section-padding py-16"><section class="mb-24 pt-8"><div class="flex flex-col md:flex-row items-center md:items-start gap-12"><div class="relative w-48 h-48 md:w-56 md:h-56 flex-shrink-0"><div class="w-full h-full rounded-full bg-gradient-to-br from-orange-200 to-amber-100 p-1 shadow-lg"><div class="w-full h-full rounded-full bg-gradient-to-br from-orange-400 to-amber-500 flex items-center justify-center text-white shadow-sm"><span class="text-4xl md:text-5xl font-serif font-medium">JS</span></div></div></div><div class="flex-1 text-center md:text-left space-y-4"><div><h1 class="text-3xl md:text-4xl lg:text-5xl font-serif font-semibold mb-3 text-foreground">Juyoung Suk</h1><p class="text-lg md:text-xl text-accent font-medium mb-4">M.S. Student in Artificial Intelligence</p></div><p class="text-base leading-relaxed text-muted-foreground max-w-2xl">I am a graduate student at KAIST researching machine learning and artificial intelligence, advised by Minjoon Seo. My work focuses on language model evaluation, multilingual AI systems, and developing novel approaches to deep learning architectures.</p><div class="flex flex-col sm:flex-row gap-3 mt-6"><button class="px-6 py-2.5 bg-accent text-accent-foreground rounded-md font-medium hover:bg-accent/90 transition-colors duration-200">Get in Touch</button><button class="px-6 py-2.5 border border-border text-foreground rounded-md font-medium hover:bg-muted transition-colors duration-200">View Publications</button><a href="/juyoung-cv.pdf" download="" class="px-6 py-2.5 border border-accent text-accent rounded-md font-medium hover:bg-accent/10 transition-colors duration-200 text-center inline-flex items-center gap-2"><svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>Download CV</a></div></div></div></section><section id="news" class="mb-20"><h2 class="text-xl font-semibold mb-8 text-foreground">News</h2><div class="space-y-4"><div class="flex gap-4 text-sm"><div class="text-muted-foreground whitespace-nowrap text-xs">2025-01</div><div class="text-foreground"><a href="https://arxiv.org/abs/2504.15431" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors">Our paper &quot;Trillion 7B Technical Report&quot; has been released on arXiv.</a></div></div><div class="flex gap-4 text-sm"><div class="text-muted-foreground whitespace-nowrap text-xs">2024-12</div><div class="text-foreground">Two papers accepted: &quot;LLM-AS-AN-INTERVIEWER&quot; and &quot;Evaluating Language Models as Synthetic Data Generators&quot;.</div></div><div class="flex gap-4 text-sm"><div class="text-muted-foreground whitespace-nowrap text-xs">2024-10</div><div class="text-foreground"><a href="https://arxiv.org/abs/2410.17578" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors">&quot;MM-Eval&quot; paper released - a multilingual meta-evaluation benchmark for LLM-as-a-Judge.</a></div></div><div class="flex gap-4 text-sm"><div class="text-muted-foreground whitespace-nowrap text-xs">2024-06</div><div class="text-foreground"><a href="https://arxiv.org/abs/2406.05761" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors">&quot;BiGGen Bench&quot; published - principled benchmark for fine-grained LLM evaluation.</a></div></div><div class="flex gap-4 text-sm"><div class="text-muted-foreground whitespace-nowrap text-xs">2024-05</div><div class="text-foreground"><a href="https://arxiv.org/abs/2405.01535" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors">&quot;Prometheus 2&quot; released - open source language model for evaluating other LLMs.</a></div></div></div></section><section id="projects" class="mb-20"><h2 class="section-heading">Projects</h2><div class="grid gap-6 md:grid-cols-2 mb-12"><div class="bg-card border border-border rounded-lg p-6 card-hover"><div class="w-full h-32 bg-gradient-to-br from-accent/10 to-accent/20 rounded-md mb-4 flex items-center justify-center"><svg class="w-8 h-8 text-accent/60" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"></path></svg></div><h3 class="font-semibold text-lg mb-2 text-foreground">Prometheus-Eval</h3><p class="text-sm text-muted-foreground mb-4 leading-relaxed">Led development of a 900+ stars open-source repository for evaluating language models using specialized LMs</p><div class="flex flex-wrap gap-2 mb-4"><span class="text-xs px-2 py-1 bg-accent/10 text-accent rounded-md font-medium">Python</span><span class="text-xs px-2 py-1 bg-accent/10 text-accent rounded-md font-medium">PyTorch</span><span class="text-xs px-2 py-1 bg-accent/10 text-accent rounded-md font-medium">HuggingFace</span><span class="text-xs px-2 py-1 bg-accent/10 text-accent rounded-md font-medium">OpenAI API</span></div><div class="flex space-x-4"><a class="text-sm text-accent hover:text-accent/80 transition-colors flex items-center gap-1.5" href="https://github.com/prometheus-eval/prometheus-eval"><svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>Code</a><a class="text-sm text-accent hover:text-accent/80 transition-colors flex items-center gap-1.5" href="/#projects"><svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg>Coming Soon</a></div></div></div><h3 class="font-semibold mb-4">Other Projects</h3><div class="grid gap-4"><div class="border-l-2 border-border pl-4 hover:border-accent transition-colors"><div class="flex items-start justify-between"><div class="flex-1"><h4 class="font-medium mb-1">Federated Learning Platform</h4><p class="text-sm text-muted-foreground mb-2">Privacy-preserving distributed learning platform for healthcare applications</p><div class="flex space-x-4"><a class="text-sm text-accent link-hover" href="https://github.com/username/fed-learning">GitHub</a></div></div></div></div><div class="border-l-2 border-border pl-4 hover:border-accent transition-colors"><div class="flex items-start justify-between"><div class="flex-1"><h4 class="font-medium mb-1">AI Code Assistant</h4><p class="text-sm text-muted-foreground mb-2">VS Code extension for AI-powered code completion and refactoring suggestions</p><div class="flex space-x-4"><a class="text-sm text-accent link-hover" href="https://github.com/username/ai-code-assistant">GitHub</a><a class="text-sm text-accent link-hover" href="/#">Demo</a></div></div></div></div></div></section><section id="publications" class="mb-20"><h2 class="section-heading">Publications</h2><div class="space-y-4"><article class="bg-card border border-border rounded-lg p-5 card-hover"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-10 h-10 bg-accent/10 rounded-lg flex items-center justify-center mt-1"><svg class="w-5 h-5 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg></div><div class="flex-1 min-w-0"><h3 class="text-base font-medium text-foreground mb-2 leading-snug hover:text-accent cursor-pointer transition-colors">Trillion 7B Technical Report</h3><p class="text-sm text-muted-foreground mb-2"><span><span>S Han<!-- -->, </span><span><strong>J Suk</strong>, </span><span>S An<!-- -->, </span><span>H Kim<!-- -->, </span><span>K Kim<!-- -->, </span><span>W Yang<!-- -->, </span><span>S Choi<!-- -->, </span><span>J Shin</span></span></p><p class="text-sm text-accent font-medium mb-3"><span class="italic">arXiv preprint arXiv:2504.15431</span> • <!-- -->2025</p><div class="flex flex-wrap gap-2"><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-accent/10 text-accent rounded-md text-sm font-medium hover:bg-accent/20 transition-colors" href="https://arxiv.org/abs/2504.15431"><svg class="w-3 h-3" fill="currentColor" viewBox="0 0 24 24"><path d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>arXiv</a><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-secondary/60 text-foreground rounded-md text-sm font-medium hover:bg-secondary transition-colors" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=mENsLCkAAAAJ&amp;citation_for_view=mENsLCkAAAAJ:qjMakFHDy7sC"><svg class="w-3 h-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.246 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path></svg>Paper</a></div></div></div></article><article class="bg-card border border-border rounded-lg p-5 card-hover"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-10 h-10 bg-accent/10 rounded-lg flex items-center justify-center mt-1"><svg class="w-5 h-5 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg></div><div class="flex-1 min-w-0"><h3 class="text-base font-medium text-foreground mb-2 leading-snug hover:text-accent cursor-pointer transition-colors">Prometheus 2: An open source language model specialized in evaluating other language models</h3><p class="text-sm text-muted-foreground mb-2"><span><span>S Kim<!-- -->, </span><span><strong>J Suk</strong>, </span><span>S Longpre<!-- -->, </span><span>BY Lin<!-- -->, </span><span>J Shin<!-- -->, </span><span>S Welleck<!-- -->, </span><span>G Neubig<!-- -->, </span><span>M Lee<!-- -->, </span><span>...</span></span></p><p class="text-sm text-accent font-medium mb-3"><span class="italic">arXiv preprint arXiv:2405.01535</span> • <!-- -->2024</p><div class="flex flex-wrap gap-2"><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-accent/10 text-accent rounded-md text-sm font-medium hover:bg-accent/20 transition-colors" href="https://arxiv.org/abs/2405.01535"><svg class="w-3 h-3" fill="currentColor" viewBox="0 0 24 24"><path d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>arXiv</a><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-secondary/60 text-foreground rounded-md text-sm font-medium hover:bg-secondary transition-colors" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=mENsLCkAAAAJ&amp;citation_for_view=mENsLCkAAAAJ:u-x6o8ySG0sC"><svg class="w-3 h-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.246 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path></svg>Paper</a></div></div></div></article><article class="bg-card border border-border rounded-lg p-5 card-hover"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-10 h-10 bg-accent/10 rounded-lg flex items-center justify-center mt-1"><svg class="w-5 h-5 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg></div><div class="flex-1 min-w-0"><h3 class="text-base font-medium text-foreground mb-2 leading-snug hover:text-accent cursor-pointer transition-colors">CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean</h3><p class="text-sm text-muted-foreground mb-2"><span><span>E Kim<!-- -->, </span><span><strong>J Suk</strong>, </span><span>P Oh<!-- -->, </span><span>H Yoo<!-- -->, </span><span>J Thorne<!-- -->, </span><span>A Oh</span></span></p><p class="text-sm text-accent font-medium mb-3"><span class="italic">arXiv preprint arXiv:2403.06412</span> • <!-- -->2024</p><div class="flex flex-wrap gap-2"><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-accent/10 text-accent rounded-md text-sm font-medium hover:bg-accent/20 transition-colors" href="https://arxiv.org/abs/2403.06412"><svg class="w-3 h-3" fill="currentColor" viewBox="0 0 24 24"><path d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>arXiv</a><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-secondary/60 text-foreground rounded-md text-sm font-medium hover:bg-secondary transition-colors" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=mENsLCkAAAAJ&amp;citation_for_view=mENsLCkAAAAJ:u5HHmVD_uO8C"><svg class="w-3 h-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.246 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path></svg>Paper</a></div></div></div></article><article class="bg-card border border-border rounded-lg p-5 card-hover"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-10 h-10 bg-accent/10 rounded-lg flex items-center justify-center mt-1"><svg class="w-5 h-5 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg></div><div class="flex-1 min-w-0"><h3 class="text-base font-medium text-foreground mb-2 leading-snug hover:text-accent cursor-pointer transition-colors">The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models</h3><p class="text-sm text-muted-foreground mb-2"><span><span>S Kim<!-- -->, </span><span><strong>J Suk</strong>, </span><span>JY Cho<!-- -->, </span><span>S Longpre<!-- -->, </span><span>C Kim<!-- -->, </span><span>D Yoon<!-- -->, </span><span>G Son<!-- -->, </span><span>Y Cho<!-- -->, </span><span>...</span></span></p><p class="text-sm text-accent font-medium mb-3"><span class="italic">arXiv preprint arXiv:2406.05761</span> • <!-- -->2024</p><div class="flex flex-wrap gap-2"><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-accent/10 text-accent rounded-md text-sm font-medium hover:bg-accent/20 transition-colors" href="https://arxiv.org/abs/2406.05761"><svg class="w-3 h-3" fill="currentColor" viewBox="0 0 24 24"><path d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>arXiv</a><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-secondary/60 text-foreground rounded-md text-sm font-medium hover:bg-secondary transition-colors" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=mENsLCkAAAAJ&amp;citation_for_view=mENsLCkAAAAJ:d1gkVwhDpl0C"><svg class="w-3 h-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.246 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path></svg>Paper</a></div></div></div></article><article class="bg-card border border-border rounded-lg p-5 card-hover"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-10 h-10 bg-accent/10 rounded-lg flex items-center justify-center mt-1"><svg class="w-5 h-5 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg></div><div class="flex-1 min-w-0"><h3 class="text-base font-medium text-foreground mb-2 leading-snug hover:text-accent cursor-pointer transition-colors">Evaluating Language Models as Synthetic Data Generators</h3><p class="text-sm text-muted-foreground mb-2"><span><span>S Kim<!-- -->, </span><span><strong>J Suk</strong>, </span><span>X Yue<!-- -->, </span><span>V Viswanathan<!-- -->, </span><span>S Lee<!-- -->, </span><span>Y Wang<!-- -->, </span><span>K Gashteovski<!-- -->, </span><span>...</span></span></p><p class="text-sm text-accent font-medium mb-3"><span class="italic">arXiv preprint arXiv:2412.03679</span> • <!-- -->2024</p><div class="flex flex-wrap gap-2"><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-accent/10 text-accent rounded-md text-sm font-medium hover:bg-accent/20 transition-colors" href="https://arxiv.org/abs/2412.03679"><svg class="w-3 h-3" fill="currentColor" viewBox="0 0 24 24"><path d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>arXiv</a><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-secondary/60 text-foreground rounded-md text-sm font-medium hover:bg-secondary transition-colors" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=mENsLCkAAAAJ&amp;citation_for_view=mENsLCkAAAAJ:2osOgNQ5qMEC"><svg class="w-3 h-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.246 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path></svg>Paper</a></div></div></div></article><article class="bg-card border border-border rounded-lg p-5 card-hover"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-10 h-10 bg-accent/10 rounded-lg flex items-center justify-center mt-1"><svg class="w-5 h-5 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg></div><div class="flex-1 min-w-0"><h3 class="text-base font-medium text-foreground mb-2 leading-snug hover:text-accent cursor-pointer transition-colors">LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation</h3><p class="text-sm text-muted-foreground mb-2"><span><span>E Kim<!-- -->, </span><span><strong>J Suk</strong>, </span><span>S Kim<!-- -->, </span><span>N Muennighoff<!-- -->, </span><span>D Kim<!-- -->, </span><span>A Oh</span></span></p><p class="text-sm text-accent font-medium mb-3"><span class="italic">arXiv preprint arXiv:2412.10424</span> • <!-- -->2024</p><div class="flex flex-wrap gap-2"><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-accent/10 text-accent rounded-md text-sm font-medium hover:bg-accent/20 transition-colors" href="https://arxiv.org/abs/2412.10424"><svg class="w-3 h-3" fill="currentColor" viewBox="0 0 24 24"><path d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>arXiv</a><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-secondary/60 text-foreground rounded-md text-sm font-medium hover:bg-secondary transition-colors" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=mENsLCkAAAAJ&amp;citation_for_view=mENsLCkAAAAJ:UeHWp8X0CEIC"><svg class="w-3 h-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.246 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path></svg>Paper</a></div></div></div></article><article class="bg-card border border-border rounded-lg p-5 card-hover"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-10 h-10 bg-accent/10 rounded-lg flex items-center justify-center mt-1"><svg class="w-5 h-5 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg></div><div class="flex-1 min-w-0"><h3 class="text-base font-medium text-foreground mb-2 leading-snug hover:text-accent cursor-pointer transition-colors">MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models</h3><p class="text-sm text-muted-foreground mb-2"><span><span>G Son<!-- -->, </span><span>D Yoon<!-- -->, </span><span><strong>J Suk</strong>, </span><span>J Aula-Blasco<!-- -->, </span><span>M Aslan<!-- -->, </span><span>VT Kim<!-- -->, </span><span>SB Islam<!-- -->, </span><span>...</span></span></p><p class="text-sm text-accent font-medium mb-3"><span class="italic">arXiv preprint arXiv:2410.17578</span> • <!-- -->2024</p><div class="flex flex-wrap gap-2"><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-accent/10 text-accent rounded-md text-sm font-medium hover:bg-accent/20 transition-colors" href="https://arxiv.org/abs/2410.17578"><svg class="w-3 h-3" fill="currentColor" viewBox="0 0 24 24"><path d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>arXiv</a><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 bg-secondary/60 text-foreground rounded-md text-sm font-medium hover:bg-secondary transition-colors" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=mENsLCkAAAAJ&amp;citation_for_view=mENsLCkAAAAJ:9yKSN-GCB0IC"><svg class="w-3 h-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.246 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path></svg>Paper</a></div></div></div></article></div></section><section id="research" class="mb-20"><h2 class="section-heading">Current Research</h2><div class="grid gap-6 md:grid-cols-2"><div class="p-6 border border-border rounded-lg hover:border-accent transition-colors"><div class="flex items-start gap-3"><span class="text-2xl">⚡</span><div><h3 class="font-semibold mb-2">Efficient Deep Learning</h3><p class="text-sm leading-relaxed text-muted-foreground">Developing methods to reduce the computational and memory requirements of deep neural networks while maintaining accuracy.</p></div></div></div><div class="p-6 border border-border rounded-lg hover:border-accent transition-colors"><div class="flex items-start gap-3"><span class="text-2xl">🔍</span><div><h3 class="font-semibold mb-2">Interpretable AI</h3><p class="text-sm leading-relaxed text-muted-foreground">Creating techniques to understand and explain the decision-making processes of complex machine learning models.</p></div></div></div><div class="p-6 border border-border rounded-lg hover:border-accent transition-colors"><div class="flex items-start gap-3"><span class="text-2xl">👁️</span><div><h3 class="font-semibold mb-2">Vision-Language Models</h3><p class="text-sm leading-relaxed text-muted-foreground">Exploring the intersection of computer vision and natural language processing for multimodal understanding.</p></div></div></div><div class="p-6 border border-border rounded-lg hover:border-accent transition-colors"><div class="flex items-start gap-3"><span class="text-2xl">🔐</span><div><h3 class="font-semibold mb-2">Federated Learning</h3><p class="text-sm leading-relaxed text-muted-foreground">Investigating privacy-preserving machine learning techniques for distributed and decentralized training scenarios.</p></div></div></div></div></section><section id="experience" class="mb-20"><h2 class="section-heading">Work Experience</h2><div class="relative"><div class="absolute left-0 md:left-1/2 transform md:-translate-x-1/2 w-0.5 h-full bg-border"></div><div class="relative flex items-center mb-12 md:flex-row"><div class="absolute left-0 md:left-1/2 transform md:-translate-x-1/2 w-4 h-4 bg-accent rounded-full border-4 border-background"></div><div class="ml-8 md:ml-0 flex-1 md:pr-12 md:text-right"><div class="bg-secondary p-6 rounded-lg"><div class="md:flex md:flex-col md:items-end"><h3 class="font-semibold text-lg mb-1">Member of Technical Staff</h3><p class="text-accent font-medium mb-1">Trillion Labs</p><p class="text-sm text-muted-foreground mb-3">Nov. 2024 - Present</p><p class="text-sm leading-relaxed mb-3">Core developer of Trillion-7B, a 7.76B parameter compute-efficient multilingual frontier model.</p><ul class="text-sm space-y-1 md:text-right"><li class="flex items-start"><span class="text-accent mr-2 md:order-2 md:ml-2 md:mr-0">•</span><span class="text-muted-foreground">Core developer of Trillion-7B (HuggingFace, NVIDIA GTC)</span></li><li class="flex items-start"><span class="text-accent mr-2 md:order-2 md:ml-2 md:mr-0">•</span><span class="text-muted-foreground">Working on pre-training and post-training stages for LLM</span></li></ul></div></div></div><div class="hidden md:block flex-1"></div></div><div class="relative flex items-center mb-12 md:flex-row-reverse"><div class="absolute left-0 md:left-1/2 transform md:-translate-x-1/2 w-4 h-4 bg-accent rounded-full border-4 border-background"></div><div class="ml-8 md:ml-0 flex-1 md:pl-12"><div class="bg-secondary p-6 rounded-lg"><div class=""><h3 class="font-semibold text-lg mb-1">Machine Learning Engineer</h3><p class="text-accent font-medium mb-1">ThetaOne</p><p class="text-sm text-muted-foreground mb-3">Feb. 2023 - July. 2023</p><p class="text-sm leading-relaxed mb-3">Engineered end-to-end ML pipeline for Metabuddy, implementing RAG architecture with LangChain.</p><ul class="text-sm space-y-1 "><li class="flex items-start"><span class="text-accent mr-2 ">•</span><span class="text-muted-foreground">Engineered end-to-end ML pipeline for Metabuddy</span></li><li class="flex items-start"><span class="text-accent mr-2 ">•</span><span class="text-muted-foreground">Implemented RAG architecture with LangChain and custom language models</span></li><li class="flex items-start"><span class="text-accent mr-2 ">•</span><span class="text-muted-foreground">Developed grammar error detection and reranker models</span></li></ul></div></div></div><div class="hidden md:block flex-1"></div></div><div class="relative flex items-center mb-12 md:flex-row"><div class="absolute left-0 md:left-1/2 transform md:-translate-x-1/2 w-4 h-4 bg-accent rounded-full border-4 border-background"></div><div class="ml-8 md:ml-0 flex-1 md:pr-12 md:text-right"><div class="bg-secondary p-6 rounded-lg"><div class="md:flex md:flex-col md:items-end"><h3 class="font-semibold text-lg mb-1">Machine Learning Engineer Intern</h3><p class="text-accent font-medium mb-1">NAVER Corp.</p><p class="text-sm text-muted-foreground mb-3">Aug. 2022 - Feb. 2023</p><p class="text-sm leading-relaxed mb-3">Enhanced hate speech detection model for AI Clean Bot 2.0 serving 40+ million users.</p><ul class="text-sm space-y-1 md:text-right"><li class="flex items-start"><span class="text-accent mr-2 md:order-2 md:ml-2 md:mr-0">•</span><span class="text-muted-foreground">Enhanced hate speech detection model for AI Clean Bot 2.0 (40+ million users)</span></li><li class="flex items-start"><span class="text-accent mr-2 md:order-2 md:ml-2 md:mr-0">•</span><span class="text-muted-foreground">Used Active Learning techniques for model improvement</span></li></ul></div></div></div><div class="hidden md:block flex-1"></div></div><div class="relative flex items-center mb-12 md:flex-row-reverse"><div class="absolute left-0 md:left-1/2 transform md:-translate-x-1/2 w-4 h-4 bg-accent rounded-full border-4 border-background"></div><div class="ml-8 md:ml-0 flex-1 md:pl-12"><div class="bg-secondary p-6 rounded-lg"><div class=""><h3 class="font-semibold text-lg mb-1">Software Engineer Intern</h3><p class="text-accent font-medium mb-1">Startup Name</p><p class="text-sm text-muted-foreground mb-3">Summer 2021</p><p class="text-sm leading-relaxed mb-3">Built full-stack applications and integrated ML models into production systems.</p><ul class="text-sm space-y-1 "><li class="flex items-start"><span class="text-accent mr-2 ">•</span><span class="text-muted-foreground">Developed RESTful APIs using Python and FastAPI</span></li><li class="flex items-start"><span class="text-accent mr-2 ">•</span><span class="text-muted-foreground">Implemented real-time data processing pipeline</span></li><li class="flex items-start"><span class="text-accent mr-2 ">•</span><span class="text-muted-foreground">Contributed to open-source ML libraries</span></li></ul></div></div></div><div class="hidden md:block flex-1"></div></div></div></section><section id="contact" class="mb-20"><h2 class="section-heading">Contact</h2><div class="prose prose-lg max-w-none"><p class="mb-4">Feel free to reach out for research collaborations or discussions.</p><div class="flex flex-col space-y-2"><div><span class="font-medium">Email:</span> <a class="text-accent link-hover" href="mailto:juyoung@kaist.ac.kr">juyoung@kaist.ac.kr</a></div><div><span class="font-medium">GitHub:</span> <a class="text-accent link-hover" href="https://github.com/scottsuk0306">github.com/scottsuk0306</a></div><div><span class="font-medium">Google Scholar:</span> <a class="text-accent link-hover" href="/#">Scholar Profile</a></div><div><span class="font-medium">LinkedIn:</span> <a class="text-accent link-hover" href="https://www.linkedin.com/in/juyoung-suk-b5175a192/">LinkedIn Profile</a></div></div></div></section></main><footer class="border-t border-border"><div class="max-width-container section-padding py-6"><p class="text-sm text-muted-foreground text-center">© <!-- -->2025<!-- --> <!-- -->Juyoung Suk<!-- -->. All rights reserved.</p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/","query":{},"buildId":"-HGjvzzIxlnwRl8I8McRs","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>